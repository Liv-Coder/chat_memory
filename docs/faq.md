# FAQ

This FAQ collects common questions and detailed answers for `chat_memory` users.

## General Questions

### What is hybrid memory and why is it useful?

Hybrid memory combines three complementary techniques:

- Short-term rolling windows for recent messages
- Summarization-based compression that condenses older parts of the
  conversation into compact summaries
- Vector-based semantic retrieval to fetch relevant historical context

This mix keeps prompts within LLM token budgets while preserving relevant
historical knowledge.

### How does `chat_memory` compare to other conversation memory solutions?

`chat_memory` emphasizes modularity and pluggability. Key differentiators:

- Designed for Dart & Flutter environments
- Clear separation between embedding generation, vector storage, summarization
  strategies, and orchestration
- Preset configurations for quick start and builders/factories for custom
  production setups

## Technical Questions

### How does summarization work?

Summarizers implement the `Summarizer` interface. The default workflow:

1. Slice long histories into chunks.
2. Summarize each chunk using the configured summarizer.
3. Optionally produce summaries-of-summaries for very long histories.

This staged approach prevents excessive token usage while retaining
information fidelity.

### Which embedding models are supported?

Any model can be used — you only need to implement `EmbeddingService`. The
package includes `SimpleEmbeddingService` for deterministic testing. For
production, implement a service that calls OpenAI, Anthropic, Google AI, or a
local model.

### What vector databases can I use?

Provided implementations:

- `InMemoryVectorStore` — useful for tests and fast prototyping
- `LocalVectorStore` — SQLite-backed persistence

You can implement `VectorStore` adapters for hosted services like Pinecone,
Qdrant, or Weaviate.

## Configuration & Presets

### Which preset should I choose?

- `development`: in-memory, fast iteration
- `production`: SQLite persistence and semantic retrieval enabled
- `performance`: aggressive summarization and higher `semanticTopK`
- `minimal`: summarization only, smallest footprint

Choose a preset as a starting point then tune `MemoryConfig` (token budgets,
`semanticTopK`, `minSimilarity`, etc.) for your application.

### How do I balance performance vs accuracy?

- Increase `semanticTopK` to consider more candidates (better recall) but
  cache and batch embeddings for throughput.
- Adjust `minSimilarity` to control retrieval strictness.
- Use `performance` preset for high-volume applications and monitor metrics.

## Integration Questions

### How do I integrate with OpenAI / Anthropic / Google AI?

Implement an `EmbeddingService` that wraps the provider's embedding endpoint
and returns float vectors with consistent dimensionality. Store vectors in a
`VectorStore` and set any provider-specific rate-limit handling or batching.

### Can I use offline / local models?

Yes. Implement `EmbeddingService` to call your local runtime. For large
models, use background workers or isolate processing to avoid blocking the
event loop.

## Data & Privacy

### Where is conversation data stored?

Depends on configuration: `InMemoryVectorStore` keeps vectors in RAM;
`LocalVectorStore` stores them in a local SQLite DB. Message content and
summaries are stored by whichever persistence implementation you use.

### Can I encrypt stored data?

Encryption is not provided out-of-the-box. Use an encrypted filesystem or
wrap the persistence layer to encrypt/decrypt payloads before writing to disk.

## Performance & Scaling

### How many messages can the system handle?

With summarization and compaction it can scale to very large histories. The
practical limit depends on your token budgets, vector-store performance, and
hardware resources.

### How do I support multi-user applications?

- Use one `MemoryManager` per user or namespace vector entries by user ID.
- For shared knowledge, include metadata filters when querying the vector
  store.

## Troubleshooting

### Semantic search returns poor matches — why?

- Make sure embeddings were generated by the same model and have the expected
  dimensionality.
- Confirm vectors were stored correctly and that the `minSimilarity` and
  `semanticTopK` settings are appropriate.
- Rebuild or re-index the vector store if corruption is suspected.

### Summaries are low quality — what should I check?

- Verify the summarizer implementation and token budget limits.
- Try a higher token budget or a different summarizer implementation.

## Testing & Development

### How do I test conversation memory behavior?

Use deterministic components (`InMemoryVectorStore`, `SimpleEmbeddingService`)
and write unit tests that seed messages and assert `buildPrompt` output,
estimated tokens, and summary contents.

### Recommended dev workflow

1. Start with `development` preset and unit tests.
2. Switch to `production` preset and test persistence and indexing.
3. Load tests focusing on embedding throughput and vector store queries.

## Advanced

### Can I implement custom summarization strategies?

Yes — implement the `Summarizer` interface and wire it into your
`SummarizationStrategy` via builder/factory.

### How do I monitor and instrument memory performance?

- Track `ConversationStats` (message counts, token usage, vector counts).
- Add metrics for embed latency, vector search latency, summarization time.
- Use these metrics to tune `semanticTopK`, token budgets, and batching.

## Where to go next

- Getting started: `docs/tutorials/getting_started.md`
- Advanced usage: `docs/tutorials/advanced_usage.md`
- API overview: `docs/api_reference.md`

If you'd like, I can transform this FAQ into a searchable site page or add
per-question code samples extracted directly from the source code. Tell me
which you'd prefer and I'll continue.
